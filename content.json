{"posts":[{"title":"4.Iris与集成学习","text":"摘要第四次机器学习课程实验仅供参考 将数据集按7：3 的比例随机划分为训练集和验证集，随机数生成器种子为学号后三位数431，并输出训练集和验证集前10行数据； 1234567891011121314151617181920212223242526from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitimport pandas as pd# 加载数据集data = load_iris()X = data.datay = data.target# 划分训练集和验证集（7:3比例）X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.3, random_state=431)# 组合特征和标签，并添加列名columns = data.feature_names + [&apos;target&apos;]train_data = pd.DataFrame(X_train, columns=data.feature_names)train_data[&apos;target&apos;] = y_trainval_data = pd.DataFrame(X_val, columns=data.feature_names)val_data[&apos;target&apos;] = y_val# 输出结果print(&quot;训练集前10行数据：&quot;)print(train_data.head(10))print(&quot;\\n验证集前10行数据：&quot;)print(val_data.head(10)) &lt;font size=1&gt;训练集前10行数据： sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) \\ 0 4.7 3.2 1.6 0.2 1 6.5 3.0 5.5 1.8 2 4.9 3.1 1.5 0.1 3 5.6 3.0 4.5 1.5 4 5.0 3.4 1.5 0.2 5 5.8 2.6 4.0 1.2 6 4.9 2.4 3.3 1.0 7 4.6 3.2 1.4 0.2 8 5.7 2.6 3.5 1.0 9 5.4 3.9 1.3 0.4 target 0 0 1 2 2 0 3 1 4 0 5 1 6 1 7 0 8 1 9 0 验证集前10行数据： sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) \\ 0 5.4 3.7 1.5 0.2 1 6.7 3.3 5.7 2.5 2 5.0 2.0 3.5 1.0 3 5.1 3.5 1.4 0.3 4 4.4 3.0 1.3 0.2 5 4.7 3.2 1.3 0.2 6 6.4 2.9 4.3 1.3 7 4.4 3.2 1.3 0.2 8 5.7 3.0 4.2 1.2 9 5.1 3.3 1.7 0.5 target 0 0 1 2 2 1 3 0 4 0 5 0 6 1 7 0 8 1 9 0 在训练集上训练决策树模型，生成如下的决策树边界 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879from sklearn.tree import DecisionTreeClassifierimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormap# 设置字体为支持中文的字体plt.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;] # 指定中文字体为黑体plt.rcParams[&apos;axes.unicode_minus&apos;] = False # 解决负号显示问题# 选择用于绘制的特征：花瓣长度（第3列）和花瓣宽度（第4列）X_train_petal = X_train[:, [2, 3]]X_val_petal = X_val[:, [2, 3]]# 训练决策树模型（使用花瓣特征）max_depth = 3tree_clf = DecisionTreeClassifier(max_depth=max_depth, random_state=431)tree_clf.fit(X_train_petal, y_train)# 生成网格数据x_min, x_max = 0, 7.2y_min, y_max = 0, 3xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))# 预测整个网格的类别Z = tree_clf.predict(np.c_[xx.ravel(), yy.ravel()])Z = Z.reshape(xx.shape)# 创建颜色映射custom_cmap = ListedColormap([&apos;#fafab0&apos;, &apos;#9898ff&apos;, &apos;#a0faa0&apos;])# 绘制决策边界plt.figure(figsize=(10, 6))plt.contourf(xx, yy, Z, alpha=0.3, cmap=custom_cmap)# 绘制训练数据点markers = (&apos;o&apos;, &apos;s&apos;, &apos;^&apos;)for idx, marker in zip(range(3), markers): plt.scatter(X_train_petal[y_train == idx, 0], X_train_petal[y_train == idx, 1], marker=marker, label=f&quot;{data.target_names[idx]} (train)&quot;)# 提取决策树阈值def get_thresholds(tree): thresholds = [] features = [] stack = [0] # 从根节点开始 while stack: node_id = stack.pop() if tree.children_left[node_id] != tree.children_right[node_id]: thresholds.append(tree.threshold[node_id]) features.append(tree.feature[node_id]) stack.append(tree.children_left[node_id]) stack.append(tree.children_right[node_id]) return thresholds, featuresthresholds, features = get_thresholds(tree_clf.tree_)# 绘制决策边界线line_styles = [&apos;-&apos;, &apos;--&apos;, &apos;:&apos;]for i, (th, feat) in enumerate(zip(thresholds, features)): if feat == 0: # 花瓣长度 plt.plot([th, th], [y_min, y_max], linestyle=line_styles[i%3], color=&apos;k&apos;, linewidth=2) elif feat == 1: # 花瓣宽度 plt.plot([x_min, x_max], [th, th], linestyle=line_styles[i%3], color=&apos;k&apos;, linewidth=2)# 添加标注和美化plt.title(f&quot;决策树(最大深度={max_depth})边界 电信2206赵连政0122209360431&quot;)plt.xlabel(&quot;Petal length (cm)&quot;)plt.ylabel(&quot;Petal width (cm)&quot;)plt.axis([x_min, x_max, y_min, y_max])plt.legend(loc=&quot;upper left&quot;)plt.grid(True, alpha=0.2)plt.show() 在训练集上训练Bagging（基学习器自选）和随机森林模型，基学习器个数为100，输出决策边界图，并分析结果差异； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475from sklearn.ensemble import BaggingClassifier, RandomForestClassifierimport matplotlib.pyplot as pltimport numpy as npfrom matplotlib.colors import ListedColormap# 设置基础参数n_estimators = 100max_depth = 3 # 保持与之前决策树相同深度random_seed = 431# 初始化模型bagging_clf = BaggingClassifier( DecisionTreeClassifier(max_depth=max_depth), n_estimators=n_estimators, random_state=random_seed)rf_clf = RandomForestClassifier( n_estimators=n_estimators, max_depth=max_depth, random_state=random_seed)# 训练模型bagging_clf.fit(X_train_petal, y_train)rf_clf.fit(X_train_petal, y_train)# 创建网格数据x_min, x_max = 0, 7.2y_min, y_max = 0, 3xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))# 定义可视化函数def plot_decision_boundary(clf, title, ax): # 预测网格 Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) # 绘制决策边界 custom_cmap = ListedColormap([&apos;#fafab0&apos;, &apos;#9898ff&apos;, &apos;#a0faa0&apos;]) ax.contourf(xx, yy, Z, alpha=0.3, cmap=custom_cmap) ax.contour(xx, yy, Z, cmap=&quot;YlGn&quot;, alpha=0.8 ) # 绘制训练数据点 markers = (&apos;o&apos;, &apos;s&apos;, &apos;^&apos;) for idx, marker in enumerate(markers): ax.scatter(X_train_petal[y_train == idx, 0], X_train_petal[y_train == idx, 1], marker=marker, label=data.target_names[idx]) # 美化设置 ax.set_title(title) ax.set_xlabel(&quot;Petal length (cm)&quot;) ax.set_ylabel(&quot;Petal width (cm)&quot;) ax.axis([x_min, x_max, y_min, y_max]) ax.legend(loc=&quot;upper left&quot;) ax.grid(True, alpha=0.2)# 创建对比图plt.figure(figsize=(18, 6))plt.suptitle(&apos;电信2206赵连政0122209360431&apos;, fontsize=16, fontweight=&apos;bold&apos;, y=1.05)# Bagging决策边界ax1 = plt.subplot(1, 2, 1)plot_decision_boundary(bagging_clf, &quot;Bagging (100 Decision Trees)&quot;, ax1)# 随机森林决策边界ax2 = plt.subplot(1, 2, 2)plot_decision_boundary(rf_clf, &quot;随机森林 (100 Trees)&quot;, ax2)plt.tight_layout()plt.show()# 验证集准确率对比print(f&quot;Bagging验证集准确率: {bagging_clf.score(X_val_petal, y_val):.3f}&quot;)print(f&quot;随机森林验证集准确率: {rf_clf.score(X_val_petal, y_val):.3f}&quot;) Bagging验证集准确率: 0.978 随机森林验证集准确率: 0.956 分别计算决策树、Bagging（基学习器自选）和随机森林模型在Iris数据集上三分类的混淆矩阵，并对三种算法的输出结果进行比较. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 计算混淆矩阵def print_confusion_matrix(y_true, y_pred, model_name): cm = confusion_matrix(y_true, y_pred) print(f&quot;{model_name} 混淆矩阵:&quot;) print(cm) print(f&quot;{model_name} 分类报告:&quot;) print(classification_report(y_true, y_pred)) return cm# 绘制混淆矩阵def plot_confusion_matrix(cm, model_name, ax): im = ax.imshow(cm, interpolation=&apos;nearest&apos;, cmap=plt.cm.Blues) ax.set_title(f&quot;{model_name} 混淆矩阵&quot;) tick_marks = np.arange(len(data.target_names)) ax.set_xticks(tick_marks) ax.set_yticks(tick_marks) ax.set_xticklabels(data.target_names) ax.set_yticklabels(data.target_names) ax.set_xlabel(&apos;预测标签&apos;) ax.set_ylabel(&apos;真实标签&apos;) # 添加数值标签 for i in range(len(data.target_names)): for j in range(len(data.target_names)): ax.text(j, i, cm[i, j], ha=&apos;center&apos;, va=&apos;center&apos;, color=&apos;white&apos; if cm[i, j] &gt; cm.max() / 2 else &apos;black&apos;) return im# 计算并打印混淆矩阵cm_tree = print_confusion_matrix(y_val, y_pred_tree, &quot;决策树&quot;)cm_bagging = print_confusion_matrix(y_val, y_pred_bagging, &quot;Bagging&quot;)cm_rf = print_confusion_matrix(y_val, y_pred_rf, &quot;随机森林&quot;)# 绘制混淆矩阵fig = plt.figure(figsize=(18, 8))gs = gridspec.GridSpec(1, 3)ax0 = fig.add_subplot(gs[0, 0])ax1 = fig.add_subplot(gs[0, 1])ax2 = fig.add_subplot(gs[0, 2])plot_confusion_matrix(cm_tree, &quot;决策树&quot;, ax0)plot_confusion_matrix(cm_bagging, &quot;Bagging&quot;, ax1)plot_confusion_matrix(cm_rf, &quot;随机森林&quot;, ax2)# 添加水平颜色条cax = fig.add_axes([0.3, 0.15, 0.4, 0.03]) # 调整位置和大小fig.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.Blues), cax=cax, orientation=&apos;horizontal&apos;)plt.tight_layout(rect=[0, 0.2, 1, 1]) # 调整布局以避免颜色条重叠plt.show() 决策树 混淆矩阵: [[19 0 0] [ 0 10 1] [ 0 1 14]] 决策树 分类报告: precision recall f1-score support 0 1.00 1.00 1.00 19 1 0.91 0.91 0.91 11 2 0.93 0.93 0.93 15 accuracy 0.96 45 macro avg 0.95 0.95 0.95 45 weighted avg 0.96 0.96 0.96 45 Bagging 混淆矩阵: [[19 0 0] [ 0 10 1] [ 0 0 15]] Bagging 分类报告: precision recall f1-score support 0 1.00 1.00 1.00 19 1 1.00 0.91 0.95 11 2 0.94 1.00 0.97 15 accuracy 0.98 45 macro avg 0.98 0.97 0.97 45 weighted avg 0.98 0.98 0.98 45 随机森林 混淆矩阵: [[19 0 0] [ 0 10 1] [ 0 1 14]] 随机森林 分类报告: precision recall f1-score support 0 1.00 1.00 1.00 19 1 0.91 0.91 0.91 11 2 0.93 0.93 0.93 15 accuracy 0.96 45 macro avg 0.95 0.95 0.95 45 weighted avg 0.96 0.96 0.96 45 C:\\Users\\86182\\AppData\\Local\\Temp\\ipykernel_11464\\1982013248.py:50: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. plt.tight_layout(rect=[0, 0.2, 1, 1]) # 调整布局以避免颜色条重叠","link":"/2025/12/18/4-Iris%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"},{"title":"第一次实验：短时时域分析","text":"摘要语音信号处理第一次实验仅供参考 一、 实验内容概述：本次实验实验前语音数据准备以及语音实验内容两个部分。实验前语音数据准备要求录制并加载语音数据文件WHUT_10k，绘制完整波形图找出并移除前后噪声段，确保录音数据质量。语音实验内容围绕语音信号处理基础，涵盖了绘制分帧加窗图、计算语音短时能量、语音短时幅度、语音短时过零率以及计算语音短时自相关。 二、 编程思路及步骤实验前语音数据准备部分，利用Lectrue1_Demo_speech_recorder.m代码加载语音数据文件，通过绘制完整波形图找出前后噪声段，进行处理，使用代码截取无噪声段区间以获得纯净音频，并保存处理后的语音数据及对应图像。语音实验内容中的分帧加窗图绘制，先读取语音文件，确定帧长20ms和帧移50%，进行分帧操作，确保列存储。提取第 45 帧后，分别生成矩形窗和 Hamming 窗函数，对第 45 帧数据加窗处理。最后绘制原始信号与加窗后信号的对比图。计算每帧信号的平方和，得到短时能量。逐帧计算信号的绝对值之和，绘制短时幅度随时间的变化曲线。按照公式$ZCR(t)=\\frac{1}{N}\\sum_{n=0}^{N-1}\\frac{1}{2}|\\mathrm{sign}(x(t+n))-\\mathrm{sign}(x(t+n-1))|$统计每帧内信号符号变化的次数。逐帧计算信号的自相关函数，绘制短时自相关随延迟时间的变化曲线。 三、 指定内容的输出结果一、 实验前语音数据准备利用Lectrue1_Demo_speech_recorder.m文件代码录制音频“武汉理工大学”，并截取无噪声段区间，获得纯净音频。 1234567891011121314151617181920212223242526272829303132333435fs = 10000; % set sampling rater = audiorecorder(fs, 24, 1); % creat a recoreder objectrecord(r); % speak into microphone...pause(4); % set recording timestop(r); % stop recordingdata0 = getaudiodata(r); % get audio datasound(data0, fs) % listen to complete recordingplot(data0)data = data0(3706: 18181);sound(data, fs) figuret = (1:length(data))/fs;plot(t, data)sound(data, fs)xlabel(&apos;时间/ s&apos;)ylabel(&apos;振幅&apos;)axis tighttitle({&apos;电信2206 赵连政 0122209360431&apos;&apos;武汉理工大学&apos;})fileName = &apos;WHUT10K2025&apos;;saveDataDir = &apos;./data2025/&apos;;saveDataName = strcat(saveDataDir, fileName);if ~exist(saveDataDir, &apos;dir&apos;) mkdir(saveDataDir)endsave(saveDataName, &apos;data&apos;, &apos;fs&apos;) % save audio data as mat format%save(&apos;A12345&apos;)saveFigureDir = &apos;./figure2025/&apos;;if ~exist(saveFigureDir, &apos;dir&apos;) mkdir(saveFigureDir)endsaveFigureName = strcat(saveFigureDir, fileName);title(&apos;png&apos;)print(saveFigureName, &apos;-dpng&apos;)title(&apos;emf&apos;)print(saveFigureName, &apos;-dmeta&apos;) 1上述图像前后各有一段近似为0的波形，该区域即为不含语音信息的噪声。3706-18181为有效音频段，以外为噪声段。波形在-1~+1之间，幅度合适。 1上述图像前后不含噪声波形，已经截取为含有“武汉理工大学”语音信号波形，图像更加紧凑。 二、语音实验内容：首先画出Ah 第20帧时域波形图，10k采样，与实验要求给出的图像进行对比：1． 分帧加窗图：画出oh语音第45帧单帧语音时域图，同时加矩形窗和hamming窗（oh语音，第45帧） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748%% 分帧加窗对比图（oh语音第45帧）clc; clear; close all;name = &apos;赵连政&apos;;class = &apos;电信2206&apos;;id = &apos;0122209360431&apos;;% 读取语音文件（文件名为&apos;oh.wav&apos;，采样率10kHz）[y, fs] = audioread(&apos;oh.wav&apos;); frame_duration = 0.02; % 帧长20msframe_length = round(frame_duration * fs);frame_shift = frame_length/2; % 50%帧移% 分帧操作（确保列存储）frames = buffer(y, frame_length, frame_length - frame_shift, &apos;nodelay&apos;);frame45 = frames(:,45); % 提取第45帧（列向量）% 生成窗函数（列向量）rect_win = ones(size(frame45));hamm_win = hamming(length(frame45));% 加窗操作frame_rect = frame45 .* rect_win;frame_hamm = frame45 .* hamm_win;% 矩形窗对比figure(&apos;Color&apos;,&apos;white&apos;);hold on;plot(frame45, &apos;b&apos;, &apos;LineWidth&apos;, 1.2);plot(frame_rect, &apos;r&apos;, &apos;LineWidth&apos;, 1);hold off;legend(&apos;原始信号&apos;, &apos;矩形窗处理&apos;);title({[name &apos; &apos; class &apos; &apos; id] &apos;Oh语音第45帧加窗对比 (矩形窗)&apos;});xlabel(&apos;采样点&apos;); ylabel(&apos;幅度&apos;);grid on;% Hamming窗对比figure(&apos;Color&apos;,&apos;white&apos;);hold on;plot(frame45, &apos;b&apos;, &apos;LineWidth&apos;, 1.2);plot(frame_hamm, &apos;m--&apos;, &apos;LineWidth&apos;, 1);hold off;legend(&apos;原始信号&apos;, &apos;Hamming窗处理&apos;);title({[name &apos; &apos; class &apos; &apos; id] &apos;Oh语音第45帧加窗对比 (Hamming窗)&apos;});xlabel(&apos;采样点&apos;);ylabel(&apos;幅度&apos;);grid on;% 添加公共标注annotation(&apos;textbox&apos;,... [0.3 0.001 0.4 0.05],... &apos;String&apos;,[&apos;语音内容: oh语音第45帧 | 采样率: &apos; num2str(fs/1000) &apos;kHz&apos;],... &apos;FitBoxToText&apos;,&apos;on&apos;,... &apos;EdgeColor&apos;,&apos;none&apos;,... &apos;HorizontalAlignment&apos;,&apos;center&apos;); 12蓝色曲线是原始信号，粉色曲线是经过 Hamming 窗处理后的信号。对于 Hamming 窗处理后的信号，其波形在帧的起始和结束处被衰减。这是因为 Hamming 窗是一种升余弦窗，窗函数在两端趋近于 0，中间部分接近 1。在采样点 0 和 200 附近，信号幅度被明显压低，相比原始信号，其起始和结束部分的波动幅度变小。 1蓝色曲线是原始信号，红色曲线是经过矩形窗处理后的信号。矩形窗处理后的信号与原始信号在时域波形上基本一致。因为矩形窗函数在整个窗口范围内值都为 1，相当于没有对信号进行加权处理，只是简单地截取了一段信号作为一帧。 2． 语音短时能量（武汉理工大学语音） 123456789101112131415161718192021222324252627282930% 1. 加载语音文件load(&apos;WHUT10K2025.mat&apos;); % 信号变量名为&apos;data&apos;，采样率为&apos;fs&apos;% 2. 设置帧参数frame_duration = 0.02; % 帧长20msframe_length = round(frame_duration * fs);frame_shift = round(frame_length/2); % 50%帧移% 3. 分帧操作 - 处理所有帧frames = buffer(data, frame_length, frame_length - frame_shift, &apos;nodelay&apos;);num_frames = size(frames, 2); % 获取总帧数% 4. 应用Hamming窗到所有帧hamm_win = hamming(frame_length); % 创建Hamming窗% 对每一帧应用窗函数for i = 1:num_frames frames(:, i) = frames(:, i) .* hamm_win;end% 5. 计算短时特征% 短时能量 (加窗后)short_time_energy = sum(frames.^2, 1);% 6. 创建时间轴time_signal = (0:length(data)-1)/fs; % 原始信号时间轴frame_time = ((0:num_frames-1)*frame_shift + frame_length/2)/fs; % 帧中心时间轴% 短时能量figure()plot(frame_time, short_time_energy, &apos;r&apos;, &apos;LineWidth&apos;, 1.5);title(&apos;短时能量 (Hamming窗, 帧长20ms, 帧移50%)&apos;, &apos;FontSize&apos;, 12);xlabel(&apos;时间 (s)&apos;, &apos;FontSize&apos;, 10);ylabel(&apos;能量&apos;, &apos;FontSize&apos;, 10);xlim([0 max(time_signal)]);grid on; 1短时能量曲线呈现出两个明显的高能量峰值区段，分别位于大约 0.25-0.5 秒和 0.95-1.2 秒附近，表明在这两个时间段内语音能量较强，对应着语音中的较强发音或音节。根据图中标注的语音内容为“武汉理工大学”，可以推测这两个高能量区段分别对应着“武汉”和“理工大学”这两个主要的词汇部分。 3． 语音短时幅度 （武汉理工大学语音） 12345678910% 短时幅度 (加窗后)short_time_magnitude = sum(abs(frames), 1);% 短时幅度figure()plot(frame_time, short_time_magnitude, &apos;m&apos;, &apos;LineWidth&apos;, 1.5);title(&apos;短时幅度 (Hamming窗, 帧长20ms, 帧移50%)&apos;, &apos;FontSize&apos;, 12);xlabel(&apos;时间 (s)&apos;, &apos;FontSize&apos;, 10);ylabel(&apos;幅度&apos;, &apos;FontSize&apos;, 10);xlim([0 max(time_signal)]);grid on; 1曲线中有两个主要的高幅度区段，分别位于大约 0.25-0.5 秒 和 0.95-1.2 秒，这两个区段的幅度显著高于其他部分。0-0.25 秒幅度较低，变化较为平缓，可能对应语音的起始部分或弱音。0.5-0.95 秒幅度较低且变化较小，可能对应语音中的停顿或静音段。1.2-1.4 秒幅度再次上升后逐渐下降，可能对应语音的结束部分。 4． 语音短时过零率 （武汉理工大学语音） 1234567891011121314151617% 短时过零率 (加窗后)zero_crossing_rate = zeros(1, num_frames);for i = 1:num_frames frame = frames(:, i); signs = sign(frame); % 计算符号变化次数（过零次数） cross_points = find(diff(signs) ~= 0); zero_crossing_rate(i) = length(cross_points);end% 短时过零率figure()plot(frame_time, zero_crossing_rate, &apos;g&apos;, &apos;LineWidth&apos;, 1.5);title(&apos;短时过零率 (Hamming窗, 帧长20ms, 帧移50%)&apos;, &apos;FontSize&apos;, 12);xlabel(&apos;时间 (s)&apos;, &apos;FontSize&apos;, 10);ylabel(&apos;过零次数&apos;, &apos;FontSize&apos;, 10);xlim([0 max(time_signal)]);grid on; 1曲线中有多个高峰和低谷，表明语音信号在某些时间段内过零次数较高，而在其他时间段内过零次数较低。0.2秒到0.4秒之间：过零率出现多个高峰，过零次数在60到80之间波动。这表明在这一时间段内，语音信号的过零次数较高，可能对应清音部分。1.0秒到1.2秒之间：过零率出现一个显著的高峰，过零次数超过120。这是图中过零率最高的区域，表明这一时间段内的语音信号变化非常剧烈，可能是连续的清音或语音的突变部分。 5． 语音短时自相关 （oh语音，第45帧） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162%% 短时自相关分析（第45帧）% 计算加Hamming窗后的自相关函数N = length(frame_hamm); % 帧长max_lag = N - 1; % 最大延迟% 计算自相关函数autocorr = zeros(2*max_lag+1, 1);for lag = -max_lag:max_lag n_start = max(1, 1+lag); n_end = min(N, N+lag); autocorr(lag+max_lag+1) = sum(frame_hamm(n_start:n_end) .* ... frame_hamm(n_start-lag:n_end-lag));end% 归一化自相关函数autocorr = autocorr / max(autocorr);% 提取正延迟部分（0到max_lag）positive_lags = 0:max_lag;positive_autocorr = autocorr(max_lag+1:end);% 查找基音周期（最大峰值位置，排除0延迟）[~, max_idx] = max(positive_autocorr(21:end)); % 跳过前20个样本（避免基音倍频错误）pitch_period = max_idx + 20; % 基音周期（样本数）pitch_freq = fs / pitch_period; % 基音频率（Hz）% 创建自相关图lags = -max_lag:max_lag;figure(&apos;Color&apos;,&apos;white&apos;, &apos;Position&apos;, [100, 100, 1200, 500]);% 完整自相关函数subplot(1,2,1);plot(lags, autocorr, &apos;b&apos;, &apos;LineWidth&apos;, 1.5);hold on;plot([0, 0], ylim, &apos;k--&apos;, &apos;LineWidth&apos;, 1); % 零延迟线title({[name &apos; &apos; class &apos; &apos; id] &apos;Oh语音第45帧短时自相关函数 (Hamming窗)&apos;});xlabel(&apos;延迟 (样本)&apos;);ylabel(&apos;归一化自相关值&apos;);xlim([-max_lag, max_lag]);grid on;text(0.05*max_lag, 0.9, [&apos;基音周期: &apos; num2str(pitch_period) &apos; 样本&apos;], &apos;FontSize&apos;, 10);text(0.05*max_lag, 0.8, [&apos;基音频率: &apos; num2str(round(pitch_freq)) &apos; Hz&apos;], &apos;FontSize&apos;, 10);% 局部放大图（重点显示基音周期）subplot(1,2,2);plot(positive_lags, positive_autocorr, &apos;b&apos;, &apos;LineWidth&apos;, 1.5);hold on;plot([pitch_period, pitch_period], ylim, &apos;r--&apos;, &apos;LineWidth&apos;, 1.5); % 基音周期线title(&apos;自相关函数局部放大 (0-300样本)&apos;);xlabel(&apos;延迟 (样本)&apos;);ylabel(&apos;归一化自相关值&apos;);xlim([0, 300]);grid on;text(pitch_period+5, 0.9, [&apos;基音周期: &apos; num2str(pitch_period) &apos; 样本&apos;], &apos;FontSize&apos;, 10, &apos;Color&apos;, &apos;r&apos;);% 添加公共标注annotation(&apos;textbox&apos;,... [0.3 0.001 0.4 0.05],... &apos;String&apos;,[&apos;帧长: &apos; num2str(frame_length) &apos; 样本 | 基音频率: &apos; num2str(round(pitch_freq)) &apos; Hz&apos;],... &apos;FitBoxToText&apos;,&apos;on&apos;,... &apos;EdgeColor&apos;,&apos;none&apos;,... &apos;HorizontalAlignment&apos;,&apos;center&apos;); 1在延迟为0的位置，自相关值达到最大值1，这是自相关函数的典型特征，因为信号与自身的完全重合时相关性最强。随着延迟的增加或减少，自相关值呈现出周期性的波动。在延迟约为±50样本的位置，自相关值形成了明显的峰值。右图中在延迟为0的位置，自相关值为1。随着延迟的增加，自相关值呈现出周期性的波动。在延迟约为50样本的位置，自相关值形成了一个明显的峰值，这与左图中标注的基音周期一致。 四、 编程中遇到的问题、解决方法及存在的疑问 在计算语音信号的短时能量、短时幅度、短时过零率和短时自相关等特征时，因为公式的理解不准确、计算过程中的数值精度问题或边界处理不当等原因，导致计算结果与预期不符。解决方法：仔细研读相关公式，确保对每个特征的定义和计算方法有准确的理解。例如，在计算短时过零率时，要注意符号变化的判断条件以及如何处理连续的零值等情况。在编程实现特征计算时，要注意数值精度的问题。可以采用合适的数值类型（如双精度浮点数）来存储和计算数据，以减少舍入误差等对结果的影响。对于分帧后的语音信号，在计算每帧的特征值时，要注意边界处理。例如，在计算最后一帧时，可能会出现数据不足的情况，可以通过补零或舍弃不完整的帧等方法来解决。编写代码后，可以使用一些已知结果的测试数据来验证所编写代码的正确性。例如，可以构造一个简单的正弦信号，计算其短时能量和短时自相关等特征，并与理论结果进行比较，以检查代码是否存在错误。 12声明本文档部分内容由人工智能生成，不保证准确性，仅作参考使用。 参考文章:参考链接","link":"/2025/12/19/%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%EF%BC%9A%E7%9F%AD%E6%97%B6%E6%97%B6%E5%9F%9F%E5%88%86%E6%9E%90/"},{"title":"Hadoop的安装与伪分布式测试","text":"摘要大数据技术基础第二次实验报告仅供参考 第一部分：实验预习报告（包括实验目的、意义，实验基本原理与方法，主要仪器设备及耗材，实验方案与技术路线等） 一、实验目的与意义 掌握Hadoop安装：使学习者能够理解并独立完成Hadoop在Ubuntu系统上的安装过程。 理解Hadoop配置：让学习者了解Hadoop的配置文件和参数，以及如何根据需要修改配置。 熟悉Linux操作：通过安装过程加深对Linux系统操作的熟悉度，为后续的大数据技术学习打下基础。 实践大数据处理：通过Hadoop的安装与配置，为实际的大数据处理任务做准备 二、实验基本原理 Hadoop是一个开源的大数据处理框架，它允许跨多个机器使用分布式处理大数据集。Hadoop的核心是HDFS（Hadoop Distributed File System）和MapReduce编程模型。HDFS提供了一个高度可靠的存储系统，而MapReduce则提供了一个高效的数据处理模型。本教程主要基于原生Hadoop 2，包括Hadoop 2.6.0和2.7.1等版本，通过详细步骤和适当说明，帮助用户理解Hadoop的安装和配置过程。 Hadoop架构HDFS（Hadoop Distributed File System）：Hadoop的分布式文件系统，设计用于在大规模硬件集群上存储大量数据。HDFS将文件分割成多个块（默认为128MB），并将这些块分散存储在集群的不同节点上，从而提供高吞吐量的数据处理和高可靠性的数据存储。MapReduce：Hadoop的分布式处理框架，它通过Map和Reduce两个步骤来处理大规模数据集。Map步骤负责将输入数据转换为键值对，而Reduce步骤则对这些键值对进行归并和处理，以产生最终结果。 Hadoop 2.x版本引入了YARN（Yet Another Resource Negotiator），负责集群资源管理和任务调度，提高了系统的可扩展性和灵活性。 分布式存储数据复制：HDFS通过在不同节点上复制数据块来提高数据的可靠性。默认情况下，每个数据块会有三份复制，存放在不同的节点上，以防止硬件故障导致数据丢失。高吞吐量：HDFS优化了数据的读写操作，使其适合于大规模数据集的批量处理。它通过在多个节点上并行处理数据来实现高吞吐量。 分布式计算MapReduce编程模型：MapReduce允许开发者编写可以在多个节点上并行运行的Map和Reduce函数，从而实现对大规模数据集的并行处理。大规模数据处理：MapReduce框架能够有效地处理PB级别的数据集，通过将任务分配到集群中的多个节点上，实现大规模数据的快速处理。 Java环境依赖Java运行环境：Hadoop是用Java语言编写的，因此需要Java运行环境来执行Hadoop的各个组件。环境变量配置：为了使系统能够找到Java的执行路径，需要设置JAVA_HOME环境变量，指向Java安装目录。此外，还需要将Java的bin目录添加到系统的PATH变量中，以便在任何位置都能调用Java命令。 三、主要仪器设备及耗材硬件资源 计算机或服务器：至少需要一台计算机作为Hadoop的安装和运行环境，如果是进行集群配置，则需要多台计算机。 存储设备：足够的硬盘存储空间用于安装Hadoop及其数据存储。 网络设备：网卡和网络连接设备，用于节点间的通信。 软件资源 操作系统：Ubuntu 22.04 64位作为Hadoop运行的操作系统环境。 Java开发包（JDK）：Hadoop需要Java环境来运行，因此需要安装JDK。 SSH服务：用于无密码登录和远程管理Hadoop集群。 Apache Hadoop：Hadoop的安装包，可以从Apache官网或镜像站点下载。 文本编辑器： gedit，用于编辑配置文件。 四、实验方案与技术路线 环境搭建：在Ubuntu系统上创建hadoop用户，为Hadoop安装和配置做准备。 系统更新：更新apt包管理器，确保软件包的最新状态。 依赖安装：安装SSH服务以实现无密码登录，安装Java环境作为Hadoop运行的基础。 SSH无密码登录配置：配置SSH密钥，实现本机无密码登录。 Hadoop安装：下载Hadoop安装包，解压并设置环境变量。 单机模式配置：Hadoop默认支持单机模式，无需额外配置即可运行。 伪分布式配置：修改core-site.xml和hdfs-site.xml配置文件。格式化NameNode。启动Hadoop的NameNode和DataNode守护进程。 运行Hadoop实例：在HDFS中创建用户目录。将配置文件复制到HDFS中。运行MapReduce示例程序。 YARN配置（可选）：修改mapred-site.xml和yarn-site.xml配置文件。启动YARN和历史服务器。 PATH环境变量配置：将Hadoop的可执行文件路径添加到PATH环境变量中，方便命令行操作。 第二部分：实验过程记录（包括实验原始数据记录，实验现象记录，实验过程发现的问题等） 一、实验准备阶段 环境搭建：确保使用的是Ubuntu 64位系统。创建名为hadoop的新用户，给予必要的权限。 更新系统包：执行sudo apt-get update命令更新系统包。 安装SSH服务：安装openssh-server以实现无密码登录。 安装Java环境：选择一种方式安装JDK，并配置JAVA_HOME环境变量。 下载Hadoop：从官方网站或提供的百度云盘链接下载Hadoop安装包。 解压Hadoop：将下载的Hadoop包解压到/usr/local/目录下，并重命名文件夹为hadoop。 二、Hadoop单机配置 非分布式模式运行：直接运行Hadoop附带的例子程序，体验单机模式下的MapReduce作业。 配置HDFS：修改core-site.xml和hdfs-site.xml配置文件，设置HDFS的存储路径和文件系统。 格式化NameNode：执行hdfs namenode -format命令格式化HDFS文件系统。 启动Hadoop守护进程：执行start-dfs.sh脚本启动Hadoop的NameNode和DataNode。 三、Hadoop伪分布式配置 修改配置文件：根据伪分布式的需求，修改core-site.xml和hdfs-site.xml配置文件。 启动Hadoop：再次执行start-dfs.sh脚本启动Hadoop。 运行MapReduce作业：将数据上传到HDFS，并在HDFS上运行MapReduce作业。 查看结果：使用hdfs dfs -cat命令查看MapReduce作业的输出结果。 第三部分 结果与讨论一、实验结果分析 验证Hadoop运行状态：使用jps命令查看Hadoop守护进程是否成功启动。 验证MapReduce作业：检查MapReduce作业的输出结果是否符合预期。 访问HDFS：使用Hadoop的文件系统命令在HDFS上进行文件操作，验证HDFS的功能。具体结果图片见第二部分实验过程。 二、小结、建议及体会在本次实验中，我们成功地安装并配置了Hadoop，从单机模式到伪分布式模式的转换让我们深入理解了Hadoop的架构，包括HDFS和MapReduce的工作原理，以及它们如何实现大规模数据的分布式存储和处理。实验过程中，我们学习了Linux环境下的关键操作，如用户管理、软件包更新、SSH无密码登录配置、Java环境安装和环境变量配置。此外，我们还探索了YARN资源管理器，它是Hadoop 2.x版本中负责资源管理和任务调度的重要组件。建议在实际操作之前，通过书籍、在线课程或官方文档了解Hadoop的基本概念和架构，这将有助于更好地理解安装过程中的每个步骤。尝试安装不同版本的Hadoop，以了解它们之间的差异和兼容性问题，也是一个很好的学习方式。在掌握单机和伪分布式配置后，搭建一个小型的Hadoop集群可以进一步加深对分布式计算的理解。遇到问题时，学会查看日志文件、使用搜索引擎寻找解决方案，以及参与社区讨论，这些都是宝贵的技能。在生产环境中部署Hadoop时，还需要考虑安全性问题，如Kerberos认证和数据加密等。通过次实验，我深刻体会到了大数据技术的深度。Hadoop的安装和配置涉及多个技术层面，从Linux系统管理到Java环境配置，再到Hadoop本身的架构理解。实验过程中遇到的问题，如环境变量配置错误、SSH连接问题等，提高了我的问题解决能力。我也意识到了持续学习的重要性，因为Hadoop和大数据领域不断发展，新技术和工具层出不穷。理论知识是基础，但亲自动手实践才能真正理解技术的精髓。在遇到难题时，我经常求助于在线社区和论坛，让我认识到了社区支持的力量。这次实践不仅加深了我对Hadoop的认识，也增强了我对未来技术挑战的信心。 参考文章:参考链接","link":"/2025/12/23/Hadoop%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%B5%8B%E8%AF%95/"},{"title":"大数据环境的安装和部署","text":"摘要大数据技术基础第一次实验报告仅供参考 第一部分：实验预习报告（包括实验目的、意义，实验基本原理与方法，主要仪器设备及耗材，实验方案与技术路线等） 一、实验目的与意义 掌握大数据技术栈：通过实验，熟悉并掌握常见的大数据处理平台和技术栈，了解其工作原理和使用方法。 理解分布式计算和存储原理：学习分布式文件系统（HDFS）的存储原理、MapReduce 和 Spark 的分布式计算模型，掌握数据在大数据环境中的流转和处理方式。 提高集群管理和资源调度能力：通过搭建管理大数据集群，掌握集群资源管理工具（YARN）的配置与调度原理，增强集群监控、优化和故障排除的能力。 培养故障处理与调试能力：在大数据环境中，学习如何排查和修复常见的部署和运行问题，提升问题解决能力。 实践大数据分析与处理流程：通过实践数据分析和处理任务，掌握如何通过 MapReduce 或 Spark 进行大规模数据计算和分析。 二、实验基本原理 虚拟机（VM，Virtual Machine）是一种软件模拟的计算机，它可以在物理硬件上运行多个操作系统，具有与实际计算机相同的功能。虚拟机通过虚拟化技术将计算机的硬件资源（如 CPU、内存、硬盘、网络接口等）分割成多个虚拟环境，允许多个操作系统并行运行。每个虚拟机在运行时都被当作独立的计算机，操作系统和应用程序可以在其中自由运行和处理任务。 虚拟化技术让用户可以在一台物理计算机（也称为宿主机）上创建多个虚拟计算机（称为虚拟机或客机），每个虚拟机都可以运行独立的操作系统和应用程序。这种技术不仅提高了硬件资源的利用率，还大大降低了硬件成本。 三、主要仪器设备及耗材 虚拟机软件： VirtualBox，一款功能强大、性能优异、简单易用的开源免费虚拟机软件，可虚拟多种操作系统，包括Windows、Mac OS X、Linux等。 实验环境： 在Windows系统上安装VirtualBox，然后在其中安装Ubuntu操作系统。 四、实验方案与技术路线 环境搭建：在Windows系统上使用VirtualBox创建Ubuntu 16.04的Linux虚拟机。 系统安装：安装Ubuntu操作系统，并进行基本配置，如网络、存储等。 核心概念学习：深入理解Linux操作系统的核心概念和命令行工具。 大数据技术准备：在Ubuntu虚拟机上安装Java环境，为后续的大数据技术如Hadoop做准备。 实践操作：通过实际操作，掌握大数据技术的原理和应用，增强技术领悟力和问题解决能力。 第二部分：实验过程记录（包括实验原始数据记录，实验现象记录，实验过程发现的问题等）由于我的电脑已经安装Ubuntu与Windows双系统，下面给出一种在Windows中使用虚拟机安装Linux系统的步骤。打开VirtualBox虚拟机官网： 点击Windows hosts按钮： 从中科大、阿里等镜像网站下载Linux系统的iso文件。 打开VirtualBox，新建系统，自定义名字和虚拟机的存放文件夹。 虚拟光盘选择ubuntu.iso文件夹。 自定义虚拟机名称和存放位置。选择虚拟光盘为下载的Ubuntu iso文件。设置内存大小至少为4096MB（4GB），处理器选择1 CPU，其他设置保持默认。进入虚拟硬盘空间选择界面，选择10GB空间。完成系统摘要界面的设置后，点击完成新建虚拟机。 点击启动虚拟机，等待系统自动打开Ubuntu安装界面。选择中文语言，并点击下载并安装Ubuntu按钮。继续后续安装步骤，直到进入时区选择界面，选择上海时区。进入用户设置界面，姓名和计算机名使用英文名以避免路径错误。系统进入自动安装界面，等待安装完成。安装完成后，验证Ubuntu系统能够正常打开终端，确保系统成功安装并可正常运行。 第三部分 结果与讨论一、实验结果分析能够正常打开终端，Ubuntu系统安装成功 二、小结、建议及体会通过这次实验，我掌握了如何在Windows操作系统中使用VirtualBox安装Ubuntu虚拟机。在安装过程中，我意识到一些看似微小的技术细节对实验的成败至关重要，例如如何合理分配虚拟机的内存，以及如何正确配置系统启动等。这些细节需要细心处理，才能确保虚拟机的正常运行。此外，这次实验让我更加了解了不同操作系统之间的协作。通过在Windows上安装Linux虚拟机，我能够直观地看到两个不同操作系统如何共同工作，这种跨平台的操作经验无论是在未来的学习还是工作中都十分有用。实验还激发了我对大数据技术的兴趣，我计划深入研究这个领域。与此同时，我也意识到，随着技术的快速发展，持续学习至关重要。而团队合作精神也是成功的关键，这不仅有助于个人成长，也对团队的协作至关重要。总的来说，这次实验不仅让我学到了新的技能，还让我对技术有了更深的理解，我期待在未来将这些经验应用到实际问题中，并为大数据技术的发展贡献自己的力量。 参考文章:参考链接","link":"/2025/12/23/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%83%A8%E7%BD%B2/"},{"title":"使用Eclipse编译运行MapReduce程序","text":"摘要大数据技术基础第三次实验仅供参考 第一部分：实验预习报告（包括实验目的、意义，实验基本原理与方法，主要仪器设备及耗材，实验方案与技术路线等） 一、实验目的与意义 理解MapReduce编程模型及其在大规模数据集并行计算中的应用。 掌握Hadoop框架的基本组成和工作原理。 学习在Eclipse集成开发环境中开发MapReduce程序的方法。 通过实践加深对大数据技术基础的理解，提高解决实际问题的能力。 二、实验基本原理 MapReduce 编程模型MapReduce是一种编程模型，用于大规模数据集的并行计算，由Google提出，后由Apache Hadoop实现。该模型包含两个主要的处理阶段：Map和Reduce。Map阶段：Map任务负责处理输入数据，将数据转换为中间键值对。Map函数接收输入数据，将其分解为一系列键值对，并输出。Reduce阶段：Reduce任务处理Map任务的输出，通常对相同键的值进行聚合操作。Reduce函数接收来自Map任务的中间键值对，并将具有相同键的值合并成最终结果。 Hadoop 框架Hadoop是一个开源框架，实现了MapReduce模型，允许用户在普通的硬件集群上运行MapReduce程序。Hadoop由两个主要部分组成：HDFS（Hadoop Distributed File System）：一个高度可靠的存储系统，用于存储大规模数据集。Hadoop MapReduce：一个分布式处理系统，用于处理HDFS上的数据。 Eclipse 集成开发环境Eclipse是一个开源的集成开发环境（IDE），支持多种编程语言，包括Java。为了在Eclipse中开发MapReduce程序，需要安装和配置以下插件： Eclipse IDE：基本的开发环境。 M2Eclipse 插件：用于管理Maven依赖。 Hadoop 插件：用于在Eclipse中运行和调试Hadoop程序。 开发 MapReduce 程序在Eclipse中开发MapReduce程序的步骤如下： 创建Maven项目：在Eclipse中创建一个新的Maven项目，用于管理项目依赖。 添加Hadoop依赖：在项目的pom.xml文件中添加Hadoop的依赖。 编写MapReduce代码：创建Java类，实现MapReduce的Mapper和Reducer接口。 打包程序：使用Maven将项目打包成JAR文件。 配置Hadoop环境：确保Hadoop环境已正确配置，并且HDFS上存在输入数据。 Hadoop 2.6.0 环境配置Hadoop 2.6.0集群的安装与配置涉及到下载Hadoop并解压缩、配置环境变量、配置Hadoop相关配置文件如core-site.xml和hdfs-site.xml等。这些配置文件定义了Hadoop集群的基本属性，如文件系统的默认名称、临时目录的位置、HDFS的副本策略等。 MapReduce 框架原理MapReduce框架的核心包括InputFormat、Mapper、Shuffle、Reducer和OutputFormat。InputFormat负责将输入文件切片成多个InputSplit，每个InputSplit对应一个Map任务。Mapper处理输入数据并输出中间键值对。Shuffle阶段包含排序、分区、压缩和合并等操作。Reducer处理Mapper的输出，并将结果输出到文件系统或其他存储系统。 三、主要仪器设备及耗材 计算机：安装有Ubuntu/CentOS操作系统的计算机，用于开发和测试MapReduce程序。 Hadoop环境：Hadoop 2.6.0版本，用于搭建分布式计算环境。 Eclipse IDE：集成开发环境，用于编写和调试MapReduce程序。 Hadoop-Eclipse-Plugin：Eclipse插件，用于在Eclipse中直接操作HDFS文件系统和运行MapReduce程序。 四、实验方案与技术路线1. 实验方案设计 环境准备：o 安装Java开发环境（JDK）。o 下载并安装Eclipse IDE for Java Developers。o 安装Hadoop 2.6.0，并按照官方文档进行基本配置，确保HDFS和YARN服务能够正常运行。 Eclipse配置：o 在Eclipse中安装M2Eclipse插件，以便管理Maven项目和依赖。o 安装Hadoop-Eclipse-Plugin，以便在Eclipse中直接操作HDFS和运行MapReduce程序。 MapReduce程序开发：o 创建一个新的Maven项目，并配置pom.xml文件，添加Hadoop作为依赖o 编写MapReduce程序，实现具体的业务逻辑，例如WordCount程序。o 将Hadoop配置文件（如core-site.xml、hdfs-site.xml和log4j.properties）复制到项目的资源目录下，以确保程序能够正确读取Hadoop配置。 程序打包与部署：o 使用Maven工具将项目打包成JAR文件。o 将打包好的JAR文件上传到HDFS上，或者在Eclipse中配置运行参数，直接在本地运行。 运行与测试：o 在Eclipse中配置MapReduce程序的运行参数，如输入路径和输出路径。o 运行MapReduce程序，并监控程序的执行状态。o 检查HDFS上的输出路径，验证程序的输出结果是否正确。 2. 技术路线 Hadoop技术栈：o 利用Hadoop生态系统，包括HDFS和MapReduce，进行大规模数据集的存储和处理。 Eclipse开发环境：o 使用Eclipse作为开发环境，通过插件扩展其功能，实现MapReduce程序的开发和调试。 Maven项目管理：o 使用Maven进行项目依赖管理，确保项目构建的一致性和可移植性。 分布式计算：o 通过MapReduce模型实现数据的分布式处理，利用集群的计算能力。 数据验证与测试：o 对MapReduce程序的输出结果进行验证，确保数据处理的正确性和程序的稳定性。 第二部分：实验过程记录（包括实验原始数据记录，实验现象记录，实验过程发现的问题等）1. 安装Eclipse： 在Ubuntu系统中，通过Ubuntu软件中心搜索并安装Eclipse IDE for Java Developers版。在软件中心的搜索栏中输入“eclipse”，找到软件后点击安装。Eclipse的默认安装目录为/usr/lib/eclipse。 安装Hadoop-Eclipse-Plugin： 为了在Eclipse中编译和运行MapReduce程序，需要安装hadoop-eclipse-plugin。可以从GitHub上的hadoop2x-eclipse-plugin项目下载插件。下载后，将hadoop-eclipse-plugin-2.6.0.jar文件复制到Eclipse安装目录的plugins文件夹中，然后运行eclipse -clean命令重启Eclipse以使插件生效。 配置Hadoop-Eclipse-Plugin：在Eclipse中配置Hadoop-Eclipse-Plugin，首先选择Window菜单下的Preference，然后在弹出的窗体中选择Hadoop Map/Reduce选项，设置Hadoop的安装目录。接着，切换到Map/Reduce开发视图，最后建立与Hadoop集群的连接，在Eclipse软件右下角的Map/Reduce Locations面板中新建Hadoop Location，并配置相关参数。 配置好Hadoop-Eclipse-Plugin后，可以在Eclipse的左侧Project Explorer中查看HDFS中的文件列表，双击文件可以查看内容，右键点击可以进行上传、下载、删除等操作，无需使用hdfs dfs命令。 创建MapReduce项目：在Eclipse中创建一个新的MapReduce项目，通过File菜单选择New -&gt; Project…，选择Map/Reduce Project，填写项目名为WordCount，点击Finish完成项目创建。 在运行MapReduce程序前，需要将Hadoop的配置文件（如core-site.xml、hdfs-site.xml和log4j.properties）复制到WordCount项目下的src文件夹中，以覆盖默认参数。复制完成后，右键点击WordCount选择refresh刷新项目，然后点击工具栏中的Run图标或右键选择Run As -&gt; Run on Hadoop来运行MapReduce程序。 5. 编写MapReduce代码： 将WordCount程序的代码复制到项目中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package org.apache.hadoop.examples; import java.io.IOException;import java.util.Iterator;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser; public class WordCount { public WordCount() { } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); if(otherArgs.length &lt; 2) { System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;); System.exit(2); } Job job = Job.getInstance(conf, &quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(WordCount.TokenizerMapper.class); job.setCombinerClass(WordCount.IntSumReducer.class); job.setReducerClass(WordCount.IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); for(int i = 0; i &lt; otherArgs.length - 1; ++i) { FileInputFormat.addInputPath(job, new Path(otherArgs[i])); } FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1])); System.exit(job.waitForCompletion(true)?0:1); } public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { private IntWritable result = new IntWritable(); public IntSumReducer() { } public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException { int sum = 0; IntWritable val; for(Iterator i$ = values.iterator(); i$.hasNext(); sum += val.get()) { val = (IntWritable)i$.next(); } this.result.set(sum); context.write(key, this.result); } } public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; { private static final IntWritable one = new IntWritable(1); private Text word = new Text(); public TokenizerMapper() { } public void map(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while(itr.hasMoreTokens()) { this.word.set(itr.nextToken()); context.write(this.word, one); } } }} 运行MapReduce程序： 通过Eclipse运行WordCount程序，并设置运行参数。 第三部分 结果与讨论一、实验结果分析 成功在Ubuntu系统中安装了Eclipse IDE for Java Developers，并配置了Hadoop-Eclipse-Plugin，使得我们能够在Eclipse中直接操作HDFS文件系统和运行MapReduce程序。 创建了名为WordCount的MapReduce项目，并编写了相应的Mapper和Reducer类。程序的主要功能是统计输入文本中每个单词的出现次数。 通过Eclipse运行WordCount程序，我们设置了输入路径和输出路径。程序运行结束后，检查了HDFS上的输出路径，验证了程序的输出结果是否正确。 实验结果显示，WordCount程序正确地统计了输入文本中每个单词的出现次数，并将结果输出到了指定的HDFS目录中。 二、小结、建议及体会在本次大数据技术基础实验中，我们获得了宝贵的知识和实践经验。首先，我们深入理解了MapReduce编程模型，这一模型在大规模数据集的并行计算中扮演着核心角色。通过实验，我们不仅掌握了Hadoop框架的基本组成，包括其分布式文件系统HDFS和MapReduce编程范式，还学会了如何在Eclipse集成开发环境中高效地开发MapReduce程序，这大大增强了我们解决实际问题的能力。实验过程中，我们遇到了不少技术挑战，尤其是环境配置错误和程序调试问题。但通过查阅官方文档和在线资源，我们逐步学会了如何独立解决这些问题，这不仅提升了我们的技术能力，也增强了自主学习和问题解决的信心。对此，我们建议未来的实验者在实验前仔细阅读相关文档，深入理解Hadoop的基本概念和架构。此外，尝试安装和配置不同版本的Hadoop，以了解它们之间的差异和兼容性问题，这将是一个很好的学习实践。通过这次实验，我们深刻体会到大数据技术的深度和广度，以及持续学习的重要性。理论知识虽然重要，但亲自动手实践才能真正理解技术的精髓。实验中遇到的问题提高了我们的问题解决能力，并让我们意识到了在大数据领域中，不断学习和实践是跟上技术发展的关键。总的来说，这次实验不仅加深了我们对Hadoop和MapReduce的理解，也为我们未来在大数据领域的探索和研究打下了坚实的基础。 参考文章:参考链接","link":"/2025/12/23/%E4%BD%BF%E7%94%A8Eclipse%E7%BC%96%E8%AF%91%E8%BF%90%E8%A1%8CMapReduce%E7%A8%8B%E5%BA%8F/"},{"title":"Spark安装和编程实践（Spark2.4.0）","text":"摘要第四次大数据技术基础实验仅供参考 第一部分：实验预习报告（包括实验目的、意义，实验基本原理与方法，主要仪器设备及耗材，实验方案与技术路线等） 一、实验目的与意义 理解Spark编程模型及其在大规模数据集分布式处理中的应用。 掌握Spark框架的基本组成和工作原理。 学习在集成开发环境中开发Spark程序的方法。 通过实践加深对大数据技术基础的理解，提高解决实际问题的能力。 二、实验基本原理 Spark 概述Apache Spark 是一个开源的大数据处理通用引擎，它提供了分布式的内存抽象，使得数据处理速度相较于 Hadoop MapReduce 快100倍。Spark 提供了丰富的API，支持多种编程语言，包括Scala、Java、Python和R。 Spark 核心特性• 速度快：由于采用内存计算，Spark在处理大数据时速度远超Hadoop MapReduce• 易用性：Spark 提供了超过80个高级算子，简化复杂数据处理。• 通用性：适用于批处理、实时流处理、SQL查询和机器学习等多种数据处理场景。• 可扩展性：能够在从单个机器到数千台机器的集群上运行，处理PB级别的数据 Spark 编程模型Spark 的核心是弹性分布式数据集（RDD），它是一个不可变、分布式的数据集合，支持并行操作。RDD支持两种类型的操作：• 转换（Transformations）：如 map、filter、reduceByKey 等，它们会创建一个新的RDD。• 行动（Actions）：如 count、collect、save 等，它们会计算RDD中的所有元素。 Spark 框架组件Spark 框架包含以下主要组件：• Spark Core：提供RDD的创建、操作和动作的基本功能。• Spark SQL：提供对结构化数据的查询功能，支持SQL查询和Hive查询语言。• Spark Streaming：提供实时数据流处理的能力，可以将实时数据流转换为RDD进行处理。• MLlib：提供机器学习算法库，包括分类、回归、聚类等。• GraphX：提供对图的表示和处理能力，支持图算法和图并行计算。 Spark 运行模式Spark 可以在多种模式下运行，包括：• Local模式：在单台机器上运行，适合开发和测试。• Standalone模式：在独立的集群上运行，Spark自己管理资源。• Mesos模式：在Mesos集群上运行，可以与Hadoop共享集群资源。• YARN模式：在Hadoop YARN集群上运行，可以与Hadoop共享集群资源。 开发环境配置在Eclipse中开发Spark程序，需要安装和配置以下插件：• Eclipse IDE：基本的开发环境。• M2Eclipse 插件：用于管理Maven依赖。• Hadoop 插件：用于在Eclipse中运行和调试Hadoop程序。 三、主要仪器设备及耗材 计算机：安装有Ubuntu/CentOS操作系统的计算机，用于开发和测试MapReduce程序。 Hadoop环境：Hadoop 2.6.0版本，用于搭建分布式计算环境。 Eclipse IDE：集成开发环境，用于编写和调试MapReduce程序。 Spark环境：Apache Spark 2.4.0版本，用于实现大数据处理的通用引擎。 四、实验方案与技术路线环境准备：1. 安装Java开发环境（JDK）：确保Java JDK 1.8已安装，为Spark提供必要的运行环境。2. 下载并安装Eclipse IDE：安装Eclipse IDE for Java Developers，为开发Spark程序提供集成开发环境。3. 安装Hadoop：安装Hadoop 3.1.3，并按照官方文档进行基本配置，确保HDFS和YARN服务能够正常运行。4. 安装Spark：下载并安装Spark 2.4.0，按照文档进行基本配置，使其能够在Local模式下运行。Eclipse配置：1. 安装M2Eclipse插件：在Eclipse中安装M2Eclipse插件，以便管理Maven项目和依赖。2. 安装Hadoop-Eclipse-Plugin：在Eclipse中安装Hadoop插件，以便直接操作HDFS和运行MapReduce程序。Spark程序开发：1. 创建Maven/sbt项目：创建一个新的Maven或sbt项目，并配置pom.xml文件添加Spark作为依赖。2. 编写Spark程序：实现具体的业务逻辑，例如WordCount程序。3. 配置Hadoop环境：将Hadoop配置文件（如core-site.xml、hdfs-site.xml和log4j.properties）复制到项目的资源目录下，以确保程序能够正确读取Hadoop配置。程序打包与部署：1. 使用Maven/sbt打包：使用Maven或sbt工具将项目打包成JAR文件。2. 上传JAR文件：将打包好的JAR文件上传到HDFS上，或者在Eclipse中配置运行参数，直接在本地运行。运行与测试：1. 配置运行参数：在Eclipse中配置MapReduce程序的运行参数，如输入路径和输出路径。2. 运行程序：运行MapReduce程序，并监控程序的执行状态。3. 验证输出结果：检查HDFS上的输出路径，验证程序的输出结果是否正确技术路线Hadoop技术栈：• 利用Hadoop生态系统，包括HDFS和MapReduce，进行大规模数据集的存储和处理。Eclipse开发环境：• 使用Eclipse作为开发环境，通过插件扩展其功能，实现MapReduce程序的开发和调试。Maven/sbt项目管理：• 使用Maven或sbt进行项目依赖管理，确保项目构建的一致性和可移植性分布式计算：• 通过MapReduce模型实现数据的分布式处理，利用集群的计算能力。数据验证与测试：• 对MapReduce程序的输出结果进行验证，确保数据处理的正确性和程序的稳定性。 第二部分：实验过程记录（包括实验原始数据记录，实验现象记录，实验过程发现的问题等） Spark安装与配置• 下载Spark安装文件： 访问https://spark.apache.org/downloads.html，选择适合的版本进行下载，或直接从百度云盘下载spark-2.4.0-bin-without-hadoop.tgz文件。 解压安装包： 使用以下Shell命令将下载的文件解压到/usr/local目录下： 1234sudo tar -zxf ~/下载/spark-2.4.0-bin-without-hadoop.tgz -C /usr/local/cd /usr/localsudo mv ./spark-2.4.0-bin-without-hadoop/ ./sparksudo chown -R hadoop:hadoop ./spark 修改Spark配置文件： 复制spark-env.sh.template到spark-env.sh并编辑，在第一行添加Hadoop classpath配置： 12cd /usr/local/sparkcp ./conf/spark-env.sh.template ./conf/spark-env.sh 编辑spark-env.sh文件： 1export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath) 验证Spark安装• 运行SparkPi示例程序： 通过执行以下命令运行SparkPi示例程序，验证Spark是否安装成功： 12cd /usr/local/spark/bin./run-example SparkPi 使用grep命令过滤输出结果： 1./run-example SparkPi 2&gt;&amp;1 | grep &quot;Pi is&quot; 使用Spark Shell• 启动Spark Shell： 使用以下命令启动Spark Shell： 12cd /usr/local/spark/bin./spark-shell 加载text文件： 使用sc.textFile加载本地文件README.md： 1val textFile = sc.textFile(&quot;file:///usr/local/spark/README.md&quot;) 执行RDD操作： 进行基本的RDD操作，如获取第一行内容、计数、过滤等： 1234textFile.first()textFile.count()val lineWithSpark = textFile.filter(line =&gt; line.contains(&quot;Spark&quot;))lineWithSpark.count() 独立应用程序编程• 使用sbt对Scala独立应用程序进行编译打包：o 安装sbt： 下载并安装sbt： 123456sudo mkdir /usr/local/sbtcd ~/Downloadssudo tar -zxvf ./sbt-1.3.8.tgz -C /usr/localcd /usr/local/sbtsudo chown -R hadoop /usr/local/sbtcp ./bin/sbt-launch.jar ./ 创建sbt启动脚本并赋予权限： 12vim /usr/local/sbt/sbtchmod u+x /usr/local/sbt/sbt 编写Scala应用程序代码： 创建SimpleApp.scala文件并添加代码： SimpleApp.scala */12345678910111213import org.apache.spark.SparkContextimport org.apache.spark.SparkConfobject SimpleApp { def main(args: Array[String]) { val logFile = &quot;file:///usr/local/spark/README.md&quot; val conf = new SparkConf().setAppName(&quot;Simple Application&quot;) val sc = new SparkContext(conf) val logData = sc.textFile(logFile, 2).cache() val numAs = logData.filter(line =&gt; line.contains(&quot;a&quot;)).count() val numBs = logData.filter(line =&gt; line.contains(&quot;b&quot;)).count() println(&quot;Lines with a: %s, Lines with b: %s&quot;.format(numAs, numBs)) }} 使用sbt打包Scala程序： 在~/sparkapp目录中新建simple.sbt文件并添加依赖： 1234name := &quot;Simple Project&quot;version := &quot;1.0&quot;scalaVersion := &quot;2.11.12&quot;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.4.0&quot; 使用sbt打包应用程序： 1/usr/local/sbt/sbt package 通过spark-submit运行程序： 使用以下命令提交JAR包到Spark中运行： 1/usr/local/spark/bin/spark-submit --class &quot;SimpleApp&quot; ~/sparkapp/target/scala-2.11/simple-project_2.11-1.0.jar 过滤输出结果： 1/usr/local/spark/bin/spark-submit --class &quot;SimpleApp&quot; ~/sparkapp/target/scala-2.11/simple-project_2.11-1.0.jar 2&gt;&amp;1 | grep &quot;Lines with a:&quot; 最终结果： 1Lines with a: 62, Lines with b: 31 第三部分 结果与讨论（可加页）一、实验结果分析 Spark安装验证：通过运行SparkPi示例程序，成功得到π的近似值，确认Spark安装成功。 RDD基本操作：Spark Shell中进行的文件读取、行数统计和内容过滤等操作均返回预期结果，展示了RDD的基本处理能力。 独立应用程序运行：Scala编写的SimpleApp程序能够正确计算并输出特定字符的行数，验证了Spark API的功能和独立应用程序的运行流程。 二、小结、建议及体会在本次实验中，我们成功地安装并配置了Apache Spark 2.4.0环境，并在单机上以Local模式进行运行，通过这一过程我们深入了解了Spark的编程模型、框架组件以及在集成开发环境中开发Spark应用程序的方法。实验覆盖了从环境搭建到程序开发、打包、部署和测试的整个流程，这不仅增强了我们对Spark大数据处理能力的认识，也提升了我们的技术实践能力。通过实践，我们体会到了Spark技术在处理大规模数据集时的高效性能，同时也认识到了学习Spark技术的重要性和必要性。此外，实验过程中我们也遇到了不少挑战，尤其是在环境配置和程序调试方面。这些挑战虽然一开始让我们感到困难，但通过查阅官方文档、社区资源和不断尝试，我们逐步学会了如何独立解决问题。这个过程不仅提高了我们的技术能力，也增强了我们自主学习和问题解决的信心。我们建议未来的实验者在实验前仔细阅读相关文档，深入理解Spark的基本观念和架构，并尝试安装和配置不同版本的Spark，以了解它们之间的差异和兼容性问题。总的来说，这次实验不仅加深了我们对Spark和大数据处理技术的理解，也为我们未来在大数据领域的探索和研究打下了坚实的基础。我们深刻体会到大数据技术的深度和广度，以及持续学习的重要性。理论知识虽然重要，但亲自动手实践才能真正理解技术的精髓。实验中遇到的问题提高了我们的问题解决能力，并让我们意识到了在大数据领域中，不断学习和实践是跟上技术发展的关键。 参考文章:参考链接","link":"/2025/12/23/Spark%E5%AE%89%E8%A3%85%E5%92%8C%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%EF%BC%88Spark2-4-0%EF%BC%89/"},{"title":"OpenStack的安装与使用","text":"摘要云计算与云服务第一次实验仅供参考 一、 实验目的（1）掌握 Linux 虚拟机的安装方法；（2）掌握 OpenStack 的单机安装方法；（3）熟悉 OpenStack 的核心组件。 二、 实验环境（1）操作系统：Ubuntu-22.04 LTS（2）虚拟机软件：Vmware（3）devstack（4）OpenStack: 7.4.0 三、 实验内容与完成情况1、系统准备工作(1) 使用Vmware创建Unbuntu-22.04 LTS系统 (2) Ubuntu创建新用户由于本主机已经安装Ubuntu-Windows双系统，且版本符合要求，故直接使用Ubuntu系统完成实验。 主机名为zelazia，创建新用户并分配到sudoer用户组，能够起到一定的系统隔离作用。后续实验在ZLZ用户下完成。 2、安装必要工具(1) 安装 vim 编辑器 1sudo apt-get install vim (2) 安装 git 1sudo apt-get install git (3) 安装 pip 1sudo apt-get install python3-pip 安装完成后使用：pip --version查看版本。 (4) 安装 net-tools 1sudo apt-get install net-tools 安装完成后使用sudo ifconfig命令获取本机地址10.82.221.54 2、更换 apt 源为阿里源 12sudo mv /etc/apt/sources.list /etc/apt/sources.list.baksudo vim /etc/apt/sources.list 使用 vim (也可以使用gedit或nano编辑器)打开sources.list并添加以下内容： 1234567891011deb https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse# deb https://mirrors.aliyun.com/ubuntu/ jammy-proposed main restricted universe multiverse# deb-src https://mirrors.aliyun.com/ubuntu/ jammy-proposed main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse 对以上内容进行保存，对源进行更新，执行以下命令： 1sudo apt-get update //更新源 sudo apt-get upgrade //更新已安装的包 3、设定时间同步(1) 安装时间同步工具 sudo apt-get install ntpdate (2) 设定时区，选择 Asia、shanghaisudo dpkg-reconfigure tzdata (3) 与网络服务器同步时间并查看 sudo ntpdate cn.pool.ntp.org date 4、创建 stack 用户，并设置环境： sudo useradd -s /bin/bash -d /opt/stack -m stack 5、修改 /opt/stack 目录的权限，确保 stack 用户可以执行文件sudo chmod +x /opt/stack 6、将 stack 用户添加到 sudoers 文件，授予 stack 用户 sudo 权限 echo “stack ALL=(ALL) NOPASSWD: ALL” | sudo tee /etc/sudoers.d/stack 7、切换至 stack 用户 sudo su – stack 8、更换 pip 为清华源 mkdir .pip sudo vim .pip/pip.conf 使用 vim 添加以下内容： [global]index-url = https://pypi.tuna.tsinghua.edu.cn/simpletrusted-host = pypi.tuna.tsinghua.edu.cn 9、下载 devstack 至 devstack 文件夹，并进入devstack文件夹git clone https://opendev.org/openstack/devstackcd devstack 10、创建 local.conf 配置文件vim local.conf添加以下内容：[[local|localrc]]ADMIN_PASSWORD=secretDATABASE_PASSWORD=$ADMIN_PASSWORDRABBIT_PASSWORD=$ADMIN_PASSWORDSERVICE_PASSWORD=$ADMIN_PASSWORDHOST_IP=10.82.221.54 此步骤设置所需密码，方便后续使用。HOST_IP=后的内容修改为步骤2-(4)中记录的IP地址10.82.221.54。 11、下载 etcd 并移动到 devstack 文件夹 https://github.com/etcd-io/etcd/releases/download/v3.4.27/etcd-v3.4.27-linux-amd64.tar.gz1234```![图 22](https://cdn.jsdelivr.net/gh/Archer314/blogImage@main/img/cloudcounting/20251223221026493.png) ```mv etcd-v3.4.27-linux-amd64.tar.gz /opt/stack/devstack/files/ 12、下载 CirrOS 镜像文件并保存到 devstack 文件夹： 1wget https://download.cirros-cloud.net/0.6.2/cirros-0.6.2-x86_64-disk.img -O /opt/stack/devstack/files/cirros-0.6.2-x86_64-disk.im 13、在 devstack 目录下运行脚本 1./stack.sh 安装成功后，可用虚拟机自带的火狐浏览器打开openstack服务，网址如下：http:/ 10.82.221.54 /dashboard 根据配置文件local.conf可知用户名为admin，密码为0 四、 出现的问题与解决方案1、新用户权限问题 在刚开始建立用户ZLZ时遇到了无法使用sudo命令的问题，这是由于我在刚开始创建用户时并没有分配权限。 12su - zelaziasudo usermod -G sudo ZLZ 尝试为用户赋予sudo权限，但是并没有起到作用。 1sudo vim /etc/sudoers 尝试为用户赋予root权限，修改文件如下： 123# User privilege specificationroot ALL=(ALL) ALLusername ALL=(ALL) ALL 保存退出，但是依然没有解决问题。 log out当前用户ZLZ，回到用户zelazia，重复上述过程，完成了权限设置。2、下载缓慢 在完成最后一步./stack.sh时网络极其缓慢，我耐心等待了6.9个小时，终于成功完成了安装！ 五、 实验总结通过本次《云计算与云服务》课程中的实验一“OpenStack的安装与使用”，我对云计算平台的构建与管理有了更深刻的理解，同时也提升了在Linux环境下进行系统配置和问题解决的实践能力。实验以在Ubuntu-22.04系统上完成OpenStack的单机部署为核心目标，涵盖了从虚拟机环境搭建到复杂服务配置的全流程，让我逐步掌握了开源云计算平台的基础架构与核心组件的运行逻辑。在安装 OpenStack 的过程中，我熟悉了其单机安装的步骤和要点。从安装必要的工具如 vim、git、pip 等，到更换 apt 源以提高下载速度，再到设定时间同步确保系统时间的准确性，每一步都让我对 Linux 系统的配置和管理有了更深入的理解。特别是创建 stack 用户并设置环境，以及将该用户添加到 sudoers 文件以授予必要的权限，这些操作让我明白了在多用户系统中如何进行合理的权限管理。通过下载 devstack 并进行配置，我成功地在本地搭建了一个 OpenStack 环境。这一过程让我对 OpenStack 的架构和组件有了直观的认识，了解了各个服务之间的协作相互以及如何通过配置文件来定制化部署。然而，在实验过程中也遇到了一些问题，新用户权限不足导致无法使用 sudo 命令。这让我意识到在系统管理中，用户权限的合理分配和管理是多么重要。通过不断地尝试和查阅资料，我最终解决了这个问题，这不仅提高了我的故障排查能力，也让我对 Linux 系统的用户管理机制有了更深刻的认识。总的来说，这次实验让我在云计算与云服务的领域迈出了坚实的一步，为今后进一步学习和应用相关技术打下了坚实的基础。 参考文章:参考链接","link":"/2025/12/23/OpenStack%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"}],"tags":[{"name":"集成学习","slug":"集成学习","link":"/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"伪分布式","slug":"伪分布式","link":"/tags/%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"环境配置","slug":"环境配置","link":"/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"MapReduce","slug":"MapReduce","link":"/tags/MapReduce/"},{"name":"Eclipse","slug":"Eclipse","link":"/tags/Eclipse/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"OpenStack","slug":"OpenStack","link":"/tags/OpenStack/"}],"categories":[{"name":"课程报告","slug":"课程报告","link":"/categories/%E8%AF%BE%E7%A8%8B%E6%8A%A5%E5%91%8A/"},{"name":"机器学习实验","slug":"课程报告/机器学习实验","link":"/categories/%E8%AF%BE%E7%A8%8B%E6%8A%A5%E5%91%8A/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C/"},{"name":"语音信号处理","slug":"课程报告/语音信号处理","link":"/categories/%E8%AF%BE%E7%A8%8B%E6%8A%A5%E5%91%8A/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/"},{"name":"大数据技术基础","slug":"课程报告/大数据技术基础","link":"/categories/%E8%AF%BE%E7%A8%8B%E6%8A%A5%E5%91%8A/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%A1%80/"},{"name":"云计算与云服务","slug":"课程报告/云计算与云服务","link":"/categories/%E8%AF%BE%E7%A8%8B%E6%8A%A5%E5%91%8A/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E4%BA%91%E6%9C%8D%E5%8A%A1/"}],"pages":[{"title":"","text":"微笑墙–功能开发中–","link":"/album/index.html"},{"title":"","text":"申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请请提供：站点名称、站点链接、站点描述、logo或头像（不要设置防盗链）。 排名不分先后，刷新后重排，更新信息后请留言告知。 会定期清理很久很久不更新的、不符合要求的友链，不再另行通知。 本站不存储友链图片，如果友链图片换了无法更新。图片裂了的会替换成默认图，需要更换的请留言告知。 加载中，稍等几秒...","link":"/friend/index.html"},{"title":"","text":"碎碎念 tips：github登录后按时间正序查看、可点赞加❤️、本插件地址..「+99次查看」 碎碎念加载中，请稍等... –功能开发中–","link":"/self-talking/index.html"},{"title":"","text":"个人简介 -&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;个人信息：电子信息工程 本科 博客信息 本站推荐索引 时间轴记录","link":"/about/index.html"},{"title":"","text":"来而不往非礼也畅所欲言，有留必应","link":"/message/index.html"},{"title":"音乐歌单收藏","text":"温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"","text":"&nbsp;&nbsp;听听音乐 音乐播放器由mePlayer提供，布局参照网友博客所作，感谢作者的辛勤付出。更多音乐分享请查看歌单。 &nbsp;&nbsp;看看视频 ->点击以下条目开始播放视频,向下滑动查看更多","link":"/media/index.html"}]}